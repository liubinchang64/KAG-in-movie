import os
import time
import json
import yaml
import logging
import os
import re
from typing import List, Dict, Any, Optional, Tuple
from core.main_kb import MainKnowledgeManager
from core.temp_kb import TempKnowledgeManager
from core.llm_service import LLMService
from core.kg_loader import KnowledgeGraphLoader
from llama_index.core.node_parser import SentenceWindowNodeParser
from core.evaluator import get_checked_answer
from core.utils import ensure_dirs, visualize_retrieved_nodes, load_nodes_from_cache, init_nltk, validate_config, format_time

# å…¼å®¹å‡½æ•°ï¼šè·å–NodeWithScoreæˆ–TextNodeçš„æ–‡æœ¬å†…å®¹
def _nws_text(nws):
    if hasattr(nws, "get_text"):  # NodeWithScore æ–°æ¥å£
        return nws.get_text().strip()
    n = getattr(nws, "node", None)
    if n and hasattr(n, "get_content"):
        return n.get_content().strip()
    # å…œåº•
    return str(nws).strip()

# åˆå§‹åŒ–æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("system.log", encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)


def load_config(config_path: str = "config/config.yaml") -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        Dict[str, Any]: é…ç½®å­—å…¸
    """
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}")
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return {}


def init_services(config: Dict[str, Any]) -> Tuple[LLMService, MainKnowledgeManager, TempKnowledgeManager, SentenceWindowNodeParser, KnowledgeGraphLoader]:
    """
    åˆå§‹åŒ–å„é¡¹æœåŠ¡
    
    Args:
        config: é…ç½®å­—å…¸
    
    Returns:
        Tuple[LLMService, MainKnowledgeManager, TempKnowledgeManager, SentenceWindowNodeParser, KnowledgeGraphLoader]: åˆå§‹åŒ–çš„æœåŠ¡
    """
    # åˆå§‹åŒ–LLMæœåŠ¡
    try:
        llm = LLMService(config)
        logger.info("LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        print(f"âŒ LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        raise

    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    # åˆå§‹åŒ–nltk
    nltk_data_path = config.get("nltk_data_path", "")
    if not init_nltk(nltk_data_path):
        logger.warning("nltkåˆå§‹åŒ–ä¸å®Œå…¨ï¼Œä½†ç»§ç»­è¿è¡Œ")

    # åˆå§‹åŒ–ä¸»çŸ¥è¯†åº“
    main_kb = MainKnowledgeManager(config)
    try:
        # å°è¯•åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•
        main_meta_ret, main_review_ret, main_reranker = main_kb.get_retrievers()
        logger.info("ä¸»çŸ¥è¯†åº“ç´¢å¼•åŠ è½½æˆåŠŸ")
    except RuntimeError:
        # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œæ„å»ºç´¢å¼•
        logger.info("ä¸»çŸ¥è¯†åº“ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º")
        main_kb.build_index()
        main_meta_ret, main_review_ret, main_reranker = main_kb.get_retrievers()
        logger.info("ä¸»çŸ¥è¯†åº“ç´¢å¼•æ„å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"ä¸»åº“åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        print(f"âŒ ä¸»åº“åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        main_meta_ret, main_review_ret, main_reranker = None, None, None

    # åˆå§‹åŒ–ä¸´æ—¶çŸ¥è¯†åº“
    temp_kb = TempKnowledgeManager(config)
    try:
        temp_meta_ret, temp_review_ret, temp_reranker = temp_kb.get_retrievers()
        logger.info("ä¸´æ—¶çŸ¥è¯†åº“ç´¢å¼•åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"ä¸´åº“åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        # å°è¯•æ„å»ºä¸´æ—¶åº“ç´¢å¼•
        try:
            logger.info("å¼€å§‹æ„å»ºä¸´æ—¶åº“ç´¢å¼•")
            temp_kb.build_index()
            temp_meta_ret, temp_review_ret, temp_reranker = temp_kb.get_retrievers()
            logger.info("ä¸´æ—¶çŸ¥è¯†åº“ç´¢å¼•æ„å»ºå¹¶åŠ è½½æˆåŠŸ")
        except Exception as build_e:
            logger.error(f"ä¸´æ—¶åº“ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{build_e}")
            temp_meta_ret, temp_review_ret, temp_reranker = None, None, None

    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±åŠ è½½å™¨
    kg_loader = KnowledgeGraphLoader(config)
    try:
        # åŠ è½½çŸ¥è¯†å›¾è°±
        kg_loaded = kg_loader.load_graph(format="gexf")
        if not kg_loaded:
            # å°è¯•ä»¥JSONæ ¼å¼åŠ è½½
            kg_loaded = kg_loader.load_graph(format="json")
        if kg_loaded:
            # åŠ è½½å‘é‡å­˜å‚¨
            vs_loaded = kg_loader.load_vector_store()
            if vs_loaded:
                logger.info("çŸ¥è¯†å›¾è°±å’Œå‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ")
            else:
                logger.warning("çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸï¼Œä½†å‘é‡å­˜å‚¨åŠ è½½å¤±è´¥")
        else:
            logger.error("çŸ¥è¯†å›¾è°±åŠ è½½å¤±è´¥")
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å¤±è´¥ï¼š{e}")

    node_parser = SentenceWindowNodeParser.from_defaults(window_size=3, window_metadata_key="window")
    logger.info("èŠ‚ç‚¹è§£æå™¨åˆå§‹åŒ–æˆåŠŸ")

    return llm, main_kb, temp_kb, node_parser, kg_loader


def load_temp_retrievers(temp_kb: TempKnowledgeManager) -> Tuple[Optional[Any], Optional[Any], Optional[Any]]:
    """
    åŠ è½½ä¸´æ—¶åº“æ£€ç´¢å™¨
    
    Args:
        temp_kb: ä¸´æ—¶çŸ¥è¯†åº“ç®¡ç†å™¨
    
    Returns:
        Tuple[Optional[Any], Optional[Any], Optional[Any]]: ä¸´æ—¶åº“æ£€ç´¢å™¨
    """
    try:
        if os.path.exists(os.path.join(temp_kb.meta_index_dir, "index_store.json")) and \
           os.path.exists(os.path.join(temp_kb.review_index_dir, "index_store.json")):
            retrievers = temp_kb.get_retrievers()
            logger.info("ä¸´æ—¶åº“æ£€ç´¢å™¨åŠ è½½æˆåŠŸ")
            return retrievers
        else:
            logger.warning("ä¸´æ—¶åº“ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
            return None, None, None
    except Exception as e:
        logger.error(f"åŠ è½½ä¸´æ—¶åº“æ£€ç´¢å™¨å¤±è´¥ï¼š{e}")
        return None, None, None


def load_eval_data(eval_data_path: str = "eval_data.json") -> List[Dict[str, Any]]:
    """
    åŠ è½½è¯„æµ‹æ•°æ®
    
    Args:
        eval_data_path: è¯„æµ‹æ•°æ®æ–‡ä»¶è·¯å¾„
    
    Returns:
        List[Dict[str, Any]]: è¯„æµ‹æ•°æ®åˆ—è¡¨
    """
    try:
        if os.path.exists(eval_data_path):
            with open(eval_data_path, "r", encoding="utf-8") as f:
                eval_data = json.load(f)
            logger.info(f"æˆåŠŸåŠ è½½è¯„æµ‹æ•°æ®ï¼Œå…±{len(eval_data)}æ¡")
            return eval_data
        else:
            logger.warning(f"è¯„æµ‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{eval_data_path}")
            return []
    except Exception as e:
        logger.error(f"åŠ è½½è¯„æµ‹æ•°æ®å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ åŠ è½½è¯„æµ‹æ•°æ®å¤±è´¥ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ï¼š{e}")
        return []


def process_command(user_input: str, temp_kb: TempKnowledgeManager) -> bool:
    """
    å¤„ç†å‘½ä»¤è¡Œå‘½ä»¤
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        temp_kb: ä¸´æ—¶çŸ¥è¯†åº“ç®¡ç†å™¨
    
    Returns:
        bool: æ˜¯å¦ç»§ç»­è¿è¡Œ
    """
    if user_input.lower() == "exit":
        logger.info("ç”¨æˆ·é€€å‡ºç¨‹åº")
        return False
    elif user_input.lower() == "clear":
        # æ¸…é™¤æ§åˆ¶å°æ˜¾ç¤ºï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰
        os.system('cls' if os.name == 'nt' else 'clear')
        print("âœ… å†å²å·²æ¸…ç©º")
        logger.info("ç”¨æˆ·æ¸…ç©ºå†å²è®°å½•")
        return True
    elif user_input.lower().startswith("upload"):
        path = user_input[7:].strip()
        if os.path.exists(path):
            try:
                # æ˜¾ç¤ºä¸Šä¼ è¿›åº¦
                with tqdm(total=100, desc="ä¸Šä¼ æ–‡ä»¶") as pbar:
                    pbar.update(30)  # å‡†å¤‡é˜¶æ®µ
                    temp_kb.upload_files([path])
                    pbar.update(70)  # å®Œæˆä¸Šä¼ 
                # åé¦ˆè§£æç»“æœ
                meta_nodes = load_nodes_from_cache(temp_kb.meta_nodes_path)
                review_nodes = load_nodes_from_cache(temp_kb.review_nodes_path)
                file_size = os.path.getsize(path)/1024
                logger.info(f"ä¸Šä¼ æˆåŠŸï¼šæ–‡ä»¶ {path}ï¼Œå¤§å° {file_size:.2f}KBï¼Œè§£æå‡º {len(meta_nodes)} ä¸ªmetaèŠ‚ç‚¹å’Œ {len(review_nodes)} ä¸ªreviewèŠ‚ç‚¹")
                print(f"âœ… ä¸Šä¼ æˆåŠŸï¼šæ–‡ä»¶ç±»å‹ {os.path.splitext(path)[1]}ï¼Œå¤§å° {file_size:.2f}KB")
                print(f"ğŸ“Š è§£æç»“æœï¼šmetaèŠ‚ç‚¹ {len(meta_nodes)} æ¡ï¼ŒreviewèŠ‚ç‚¹ {len(review_nodes)} æ¡")
                # è‡ªåŠ¨è§¦å‘ç´¢å¼•æ„å»º
                print("ğŸ”„ è‡ªåŠ¨æ„å»ºä¸´æ—¶ç´¢å¼•...")
                temp_kb.build_index()
                logger.info("ä¸´æ—¶ç´¢å¼•è‡ªåŠ¨æ„å»ºå®Œæˆ")
                print("âœ… ä¸´æ—¶ç´¢å¼•è‡ªåŠ¨æ„å»ºå®Œæˆ")
            except Exception as e:
                logger.error(f"ä¸Šä¼ å¤±è´¥ï¼š{e}")
                print(f"âš ï¸ ä¸Šä¼ å¤±è´¥ï¼š{e}")
        else:
            logger.warning(f"ä¸Šä¼ è·¯å¾„ä¸å­˜åœ¨ï¼š{path}")
            print("è·¯å¾„ä¸å­˜åœ¨")
        return True
    elif user_input.lower() == "remove temp":
        temp_kb.clear()
        logger.info("ä¸´æ—¶çŸ¥è¯†å·²ç§»é™¤")
        print("ä¸´æ—¶çŸ¥è¯†å·²ç§»é™¤")
        return True
    elif user_input.lower() == "build_temp_index":
        try:
            temp_kb.build_index()
            logger.info("ä¸´æ—¶ç´¢å¼•æ„å»ºå®Œæˆ")
            print("ä¸´æ—¶ç´¢å¼•æ„å»ºå®Œæˆ")
        except Exception as e:
            logger.error(f"æ„å»ºä¸´æ—¶ç´¢å¼•å¤±è´¥ï¼š{e}")
            print(f"âš ï¸ æ„å»ºä¸´æ—¶ç´¢å¼•å¤±è´¥ï¼š{e}")
        return True
    return False


def analyze_query(llm: LLMService, user_input: str, chat_history: List[Dict[str, str]], kg_loader: Any = None) -> str:
    """
    åˆ†ææŸ¥è¯¢å¹¶ç”Ÿæˆä¼˜åŒ–çš„æ£€ç´¢æŸ¥è¯¢
    
    Args:
        llm: LLMæœåŠ¡
        user_input: ç”¨æˆ·è¾“å…¥
        chat_history: å¯¹è¯å†å²
        kg_loader: çŸ¥è¯†å›¾è°±åŠ è½½å™¨
    
    Returns:
        str: ä¼˜åŒ–åçš„æŸ¥è¯¢
    """
    # ä»çŸ¥è¯†å›¾è°±ä¸­è·å–å®ä½“ä¿¡æ¯
    kg_entity_info = ""
    if kg_loader and hasattr(kg_loader, 'G') and kg_loader.G is not None:
        try:
            # å°è¯•ä»æŸ¥è¯¢ä¸­æå–å®ä½“
            entity_name = user_input.split('çš„')[0].strip()
            logger.info(f"å°è¯•ä»æŸ¥è¯¢ä¸­æå–å®ä½“: {entity_name}")

            # æŸ¥æ‰¾åŒ¹é…çš„èŠ‚ç‚¹
            matching_nodes = []
            # å…ˆå°è¯•å®Œå…¨åŒ¹é…
            for node_id in kg_loader.G.nodes:
                if node_id == entity_name:
                    matching_nodes.append(node_id)
                    break
            # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œå°è¯•å¤§å°å†™ä¸æ•æ„Ÿçš„åŒ…å«åŒ¹é…
            if not matching_nodes:
                for node_id in kg_loader.G.nodes:
                    if entity_name.lower() in node_id.lower():
                        node_info = kg_loader.get_node_info(node_id)
                        # ä¼˜å…ˆé€‰æ‹©ç±»å‹ä¸º'person'æˆ–'movie'çš„èŠ‚ç‚¹
                        if node_info and node_info.get('type') in ['person', 'movie']:
                            matching_nodes.append(node_id)
                            break

            if matching_nodes:
                entity_info_list = []
                for node_id in matching_nodes:
                    node_info = kg_loader.get_node_info(node_id)
                    if node_info:
                        node_type = node_info.get('type', 'unknown')
                        info_str = f"å®ä½“ '{node_id}' (ç±»å‹: {node_type}):"
                        # æ·»åŠ å…³é”®å±æ€§
                        for key in ['name', 'year', 'director', 'actor', 'genre']:
                            if key in node_info:
                                info_str += f" {key}: {node_info[key]}"
                        entity_info_list.append(info_str)
                kg_entity_info = "\n".join(entity_info_list)
                logger.info(f"ä»çŸ¥è¯†å›¾è°±è·å–åˆ°å®ä½“ä¿¡æ¯: {kg_entity_info}")
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±å®ä½“æå–å¤±è´¥ï¼š{e}")

    # æ„å»ºåŒ…å«å¯¹è¯å†å²çš„æ¶ˆæ¯åˆ—è¡¨ç”¨äºæŸ¥è¯¢åˆ†æ
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªé—®é¢˜åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·å½“å‰é—®é¢˜å’Œå¯¹è¯å†å²ï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–åçš„æ£€ç´¢æŸ¥è¯¢ï¼Œå¸®åŠ©ç³»ç»Ÿæ‰¾åˆ°æœ€ç›¸å…³çš„ç”µå½±ä¿¡æ¯ã€‚\n\nä¼˜åŒ–åçš„æŸ¥è¯¢åº”:\n1. æ˜ç¡®æåˆ°æ ¸å¿ƒå®ä½“(å¦‚ç”µå½±åç§°ã€å¯¼æ¼”ã€æ¼”å‘˜ç­‰)\n2. åŒ…å«å…³é”®ä¸»é¢˜è¯\n3. ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡50å­—\n4. å¦‚æœé—®é¢˜ä¸­å·²åŒ…å«æ˜ç¡®çš„ç”µå½±åç§°ï¼Œè¯·ä¿ç•™å¹¶å¼ºåŒ–ç›¸å…³è¡¨è¿°"

    # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±å®ä½“ä¿¡æ¯ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤ºä¸­
    if kg_entity_info:
        system_prompt += f"\n\nä»¥ä¸‹æ˜¯ä»çŸ¥è¯†å›¾è°±ä¸­è·å–çš„ç›¸å…³å®ä½“ä¿¡æ¯ï¼Œå¯ä¾›å‚è€ƒ:\n{kg_entity_info}"

    analysis_messages = [{
        "role": "system",
        "content": system_prompt
    }]
    
    # æ·»åŠ æœ€è¿‘çš„5è½®å¯¹è¯å†å²ï¼ˆ10æ¡æ¶ˆæ¯ï¼‰
    recent_history = chat_history[-10:]
    analysis_messages.extend(recent_history)
    
    # æ·»åŠ å½“å‰é—®é¢˜
    analysis_messages.append({"role": "user", "content": user_input})
    
    # è°ƒç”¨LLMåˆ†æé—®é¢˜å’Œå†å²ï¼Œç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢
    try:
        analysis_response = llm.call(analysis_messages)
        translated = analysis_response["content"].strip()
        logger.info(f"ä¼˜åŒ–åçš„æ£€ç´¢æŸ¥è¯¢: {translated}")
        print(f"ğŸ” ä¼˜åŒ–åçš„æ£€ç´¢æŸ¥è¯¢: {translated}")
        return translated
    except Exception as e:
        logger.error(f"æŸ¥è¯¢åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜: {e}")
        print(f"âš ï¸ æŸ¥è¯¢åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜: {e}")
        return user_input


def retrieve_documents(translated: str, main_meta_ret: Any, main_review_ret: Any, temp_meta_ret: Any, temp_review_ret: Any) -> Tuple[List[Any], List[Any], List[Any], List[Any]]:
    """
    æ£€ç´¢æ–‡æ¡£
    
    Args:
        translated: ä¼˜åŒ–åçš„æŸ¥è¯¢
        main_meta_ret: ä¸»åº“å…ƒæ•°æ®æ£€ç´¢å™¨
        main_review_ret: ä¸»åº“è¯„è®ºæ£€ç´¢å™¨
        temp_meta_ret: ä¸´æ—¶åº“å…ƒæ•°æ®æ£€ç´¢å™¨
        temp_review_ret: ä¸´æ—¶åº“è¯„è®ºæ£€ç´¢å™¨
    
    Returns:
        Tuple[List[Any], List[Any], List[Any], List[Any]]: æ£€ç´¢ç»“æœ
    """
    t0 = time.time()
    meta_nodes = []
    try:
        if main_meta_ret:
            meta_nodes = main_meta_ret.retrieve(translated)
            logger.info(f"ä¸»åº“å…ƒæ•°æ®æ¨¡ç³ŠåŒ¹é…ç»“æœ: {len(meta_nodes)} æ¡")
            print(f"ğŸ” ä¸»åº“å…ƒæ•°æ®æ¨¡ç³ŠåŒ¹é…ç»“æœ: {len(meta_nodes)} æ¡")
    except Exception as e:
        logger.error(f"ä¸»åº“å…ƒæ•°æ®æ£€ç´¢å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ ä¸»åº“å…ƒæ•°æ®æ£€ç´¢å¤±è´¥ï¼š{e}")

    t1 = time.time()
    rev_nodes = []
    try:
        if main_review_ret:
            rev_nodes = main_review_ret.retrieve(translated)
            logger.info(f"ä¸»åº“è¯„è®ºæ¨¡ç³ŠåŒ¹é…ç»“æœ: {len(rev_nodes)} æ¡")
            print(f"ğŸ” ä¸»åº“è¯„è®ºæ¨¡ç³ŠåŒ¹é…ç»“æœ: {len(rev_nodes)} æ¡")
    except Exception as e:
        logger.error(f"ä¸»åº“è¯„è®ºæ£€ç´¢å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ ä¸»åº“è¯„è®ºæ£€ç´¢å¤±è´¥ï¼š{e}")

    t2 = time.time()
    temp_meta_nodes = []
    try:
        if temp_meta_ret:
            temp_meta_nodes = temp_meta_ret.retrieve(translated)
            logger.info(f"ä¸´åº“å…ƒæ•°æ®æ¨¡ç³ŠåŒ¹é…ç»“æœ: {len(temp_meta_nodes)} æ¡")
    except Exception as e:
        logger.error(f"ä¸´åº“å…ƒæ•°æ®æ£€ç´¢å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ ä¸´åº“å…ƒæ•°æ®æ£€ç´¢å¤±è´¥ï¼š{e}")

    t3 = time.time()
    temp_review_nodes = []
    try:
        if temp_review_ret:
            temp_review_nodes = temp_review_ret.retrieve(translated)
            logger.info(f"ä¸´åº“è¯„è®ºæ¨¡ç³ŠåŒ¹é…ç»“æœ: {len(temp_review_nodes)} æ¡")
    except Exception as e:
        logger.error(f"ä¸´åº“è¯„è®ºæ£€ç´¢å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ ä¸´åº“è¯„è®ºæ£€ç´¢å¤±è´¥ï¼š{e}")

    t4 = time.time()
    retrieval_time = format_time(t4 - t0)
    logger.info(f"æ£€ç´¢è€—æ—¶ï¼šä¸»åº“å…ƒ{format_time(t1-t0)}, ä¸»åº“è¯„{format_time(t2-t1)}, ä¸´åº“å…ƒ{format_time(t3-t2)}, ä¸´åº“è¯„{format_time(t4-t3)}, æ€»è®¡{retrieval_time}")
    print(f"æ£€ç´¢è€—æ—¶ï¼šä¸»åº“å…ƒ{t1-t0:.2f}s, ä¸»åº“è¯„{t2-t1:.2f}s, ä¸´åº“å…ƒ{t3-t2:.2f}s, ä¸´åº“è¯„{t4-t3:.2f}s")

    return meta_nodes, rev_nodes, temp_meta_nodes, temp_review_nodes


def rerank_and_answer(llm: LLMService, user_input: str, weighted_nodes: List[Tuple[Any, float]], main_reranker: Any, temp_reranker: Any, kg_loader: Any = None) -> Tuple[str, Dict[str, Any]]:
    """
    å¯¹æ£€ç´¢åˆ°çš„èŠ‚ç‚¹è¿›è¡Œé‡æ’åºå¹¶ç”Ÿæˆå›ç­”
    
    Args:
        llm: LLMæœåŠ¡
        user_input: ç”¨æˆ·è¾“å…¥
        weighted_nodes: å¸¦æƒé‡çš„èŠ‚ç‚¹åˆ—è¡¨
        main_reranker: ä¸»åº“é‡æ’åºæ¨¡å‹
        temp_reranker: ä¸´æ—¶åº“é‡æ’åºæ¨¡å‹
    
    Returns:
        Tuple[str, Dict[str, Any]]: ç”Ÿæˆçš„å›ç­”å’Œè¯¦ç»†ä¿¡æ¯
    """
    # ä½¿ç”¨é‡æ’åºæ¨¡å‹å¤„ç†èŠ‚ç‚¹
    try:
        reranked_nodes = []
        if main_reranker and weighted_nodes:
            # æå–èŠ‚ç‚¹å’Œæƒé‡
            nodes, weights = zip(*weighted_nodes)
            # å°†NodeWithScoreå¯¹è±¡è½¬æ¢ä¸ºæ–‡æœ¬å†…å®¹
            node_texts = [_nws_text(node) for node in nodes]
            # é‡æ’åº
            reranked_results = main_reranker.rerank(user_input, node_texts, top_k=10)
            # åº”ç”¨å¾—åˆ†é˜ˆå€¼è¿‡æ»¤
            filtered_results = [result for result in reranked_results if result['score'] >= main_reranker.score_threshold]
            # æå–ç´¢å¼•
            reranked_indices = [result['index'] for result in filtered_results]
            # è·å–èŠ‚ç‚¹
            reranked_nodes = [nodes[i] for i in reranked_indices]
            logger.info(f"é‡æ’åºå®Œæˆï¼ŒåŸå§‹ç»“æœæ•°: {len(reranked_results)}ï¼Œè¿‡æ»¤åç»“æœæ•°: {len(reranked_nodes)}")
        else:
            # å¦‚æœæ²¡æœ‰é‡æ’åºæ¨¡å‹æˆ–æ²¡æœ‰èŠ‚ç‚¹ï¼ŒæŒ‰æƒé‡æ’åºå¹¶å–å‰10ä¸ª
            sorted_nodes = sorted(weighted_nodes, key=lambda x: x[1], reverse=True)
            reranked_nodes = [node for node, _ in sorted_nodes[:10]]
            logger.warning("æœªä½¿ç”¨é‡æ’åºæ¨¡å‹ï¼ŒæŒ‰æƒé‡å–å‰10ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        logger.error(f"é‡æ’åºå¤±è´¥ï¼š{e}")
        # ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼ˆå–å‰10ä¸ªï¼‰
        reranked_nodes = [node for node, _ in weighted_nodes[:10]]
        print(f"âš ï¸ é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼š{e}")

    # å°è¯•ä½¿ç”¨çŸ¥è¯†å›¾è°±å¢å¼ºä¸Šä¸‹æ–‡
    kg_context = ""
    if kg_loader and hasattr(kg_loader, 'G') and kg_loader.G is not None:
        try:
            # 1. æ”¹è¿›å®ä½“æå–é€»è¾‘
            entity_name = user_input.split('çš„')[0].strip()
            logger.info(f"å°è¯•ä»æŸ¥è¯¢ä¸­æå–å®ä½“: {entity_name}")

            # 2. ç²¾ç¡®æŸ¥æ‰¾åŒ¹é…çš„èŠ‚ç‚¹ï¼ˆä¼˜å…ˆå®Œå…¨åŒ¹é…ï¼‰
            matching_nodes = []
            # å…ˆå°è¯•å®Œå…¨åŒ¹é…ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰
            for node_id in kg_loader.G.nodes:
                if node_id == entity_name:
                    matching_nodes.append(node_id)
                    break
            # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œå°è¯•å¤§å°å†™ä¸æ•æ„Ÿçš„åŒ…å«åŒ¹é…
            if not matching_nodes:
                for node_id in kg_loader.G.nodes:
                    if entity_name.lower() in node_id.lower():
                        node_info = kg_loader.get_node_info(node_id)
                        # ä¼˜å…ˆé€‰æ‹©ç±»å‹ä¸º'person'çš„èŠ‚ç‚¹ï¼ˆå¯¼æ¼”ã€æ¼”å‘˜ç­‰ï¼‰
                        if node_info and node_info.get('type') == 'person':
                            matching_nodes.append(node_id)
                            break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äººç‰©èŠ‚ç‚¹ï¼Œå†æ·»åŠ å…¶ä»–ç±»å‹
                        elif not matching_nodes:
                            matching_nodes.append(node_id)

            if matching_nodes:
                kg_info = []
                for node_id in matching_nodes:
                    # è·å–èŠ‚ç‚¹ä¿¡æ¯
                    node_info = kg_loader.get_node_info(node_id)
                    if node_info:
                        node_type = node_info.get('type', 'unknown')
                        info_str = f"å®ä½“ '{node_id}' (ç±»å‹: {node_type}):\n"
                        for key, value in node_info.items():
                            if key not in ['embedding', 'type']:  # è·³è¿‡åµŒå…¥å‘é‡å’Œç±»å‹
                                info_str += f"  {key}: {value}\n"

                        # 3. æ”¹è¿›å¯¼æ¼”-ç”µå½±å…³ç³»æŸ¥è¯¢
                        if node_type == 'person':
                            # å°è¯•å¤šç§å¯èƒ½çš„å…³ç³»ç±»å‹
                            relation_types = ["ç”±...æ‰§å¯¼", "å¯¼æ¼”", "æ‰§å¯¼"]
                            directed_movies = []
                            for rel_type in relation_types:
                                movies = kg_loader.get_related_nodes(node_id, relation_type=rel_type, node_type="movie")
                                directed_movies.extend(movies)
                                # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„ç”µå½±ï¼Œå°±åœæ­¢å°è¯•å…¶ä»–å…³ç³»ç±»å‹
                                if len(directed_movies) >= 5:
                                    break

                            if directed_movies:
                                info_str += "  æ‰§å¯¼çš„ç”µå½±:\n"
                                # å»é‡å¹¶æŒ‰å¹´ä»½æ’åºï¼ˆå¦‚æœæœ‰å¹´ä»½ä¿¡æ¯ï¼‰
                                unique_movies = {movie_id: movie_info for movie_id, movie_info in directed_movies}
                                sorted_movies = sorted(unique_movies.items(), key=lambda x: x[1].get('year', 0), reverse=True)
                                for movie_id, movie_info in sorted_movies[:5]:  # é™åˆ¶æ•°é‡
                                    movie_name = movie_info.get('name', movie_id)
                                    movie_year = movie_info.get('year', 'unknown')
                                    info_str += f"    - {movie_name} ({movie_year})\n"

                        kg_info.append(info_str)

                kg_context = "\n\n".join(kg_info)
                logger.info(f"çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œé•¿åº¦ï¼š{len(kg_context)}å­—ç¬¦")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…èŠ‚ç‚¹ï¼Œä½¿ç”¨ç›¸ä¼¼æ€§æœç´¢
                similar_nodes = kg_loader.search_similar_nodes(user_input, top_k=3)
                if similar_nodes:
                    kg_info = []
                    for node_id, score in similar_nodes:
                        # è·å–èŠ‚ç‚¹ä¿¡æ¯
                        node_info = kg_loader.get_node_info(node_id)
                        if node_info:
                            node_type = node_info.get('type', 'unknown')
                            info_str = f"ç›¸ä¼¼èŠ‚ç‚¹ '{node_id}' (ç›¸ä¼¼åº¦: {score:.2f}, ç±»å‹: {node_type}):\n"
                            for key, value in node_info.items():
                                if key != 'embedding':  # è·³è¿‡åµŒå…¥å‘é‡
                                    info_str += f"  {key}: {value}\n"
                            # å¦‚æœæ˜¯äººç‰©èŠ‚ç‚¹ï¼Œå°è¯•è·å–å…¶æ‰§å¯¼çš„ç”µå½±
                            if node_type == 'person':
                                relation_types = ["ç”±...æ‰§å¯¼", "å¯¼æ¼”", "æ‰§å¯¼"]
                                directed_movies = []
                                for rel_type in relation_types:
                                    movies = kg_loader.get_related_nodes(node_id, relation_type=rel_type, node_type="movie")
                                    directed_movies.extend(movies)
                                    if len(directed_movies) >= 3:
                                        break
                                if directed_movies:
                                    info_str += "  æ‰§å¯¼çš„ç”µå½±:\n"
                                    for movie_id, movie_info in directed_movies[:3]:
                                        movie_name = movie_info.get('name', movie_id)
                                        movie_year = movie_info.get('year', 'unknown')
                                        info_str += f"    - {movie_name} ({movie_year})\n"
                            kg_info.append(info_str)
                    kg_context = "\n\n".join(kg_info)
                    logger.info(f"çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œé•¿åº¦ï¼š{len(kg_context)}å­—ç¬¦")
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±å¢å¼ºå¤±è´¥ï¼š{e}")

    # æ„å»ºæœ€ç»ˆä¸Šä¸‹æ–‡
    node_context = "\n\n".join([_nws_text(node) for node in reranked_nodes])
    if kg_context:
        context = f"# çŸ¥è¯†å›¾è°±ä¿¡æ¯\n{kg_context}\n\n# æ–‡æ¡£æ£€ç´¢ä¿¡æ¯\n{node_context}"
    else:
        context = node_context
    logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œé•¿åº¦ï¼š{len(context)}å­—ç¬¦")

    # æ„å»ºæç¤º
    prompt = f"ç”¨æˆ·é—®é¢˜: {user_input}\n\nåŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:\n{context}\n\nè¦æ±‚:\n1. ä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œ**ä¸å¾—ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†**\n2. ä»”ç»†åˆ†æä¸Šä¸‹æ–‡ï¼Œç¡®ä¿å›ç­”ä¸é—®é¢˜ç›´æ¥ç›¸å…³\n3. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€æµç•…è‡ªç„¶\n4. ç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹è¯´æ˜ï¼Œä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜å¢å¼ºå¯è¯»æ€§\n5. å¯¹äºä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜å¹¶æä¾›å¯èƒ½çš„è§£é‡Š\n6. å¦‚æœé—®é¢˜æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­å›ç­”ï¼Œæ˜ç¡®è¯´æ˜åŸå› \n7. å›ç­”åº”ç®€æ´æ˜äº†ï¼Œé¿å…å†—ä½™ï¼ŒåŒæ—¶ç¡®ä¿ä¿¡æ¯å®Œæ•´\n8. ç›´æ¥å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ä¿¡æ¯æ”¯æŒä½ çš„å›ç­”\n9. æ€»ç»“ä¸»è¦è§‚ç‚¹ï¼Œä½¿å›ç­”æœ‰æ˜ç¡®çš„ç»“è®º\n10. å†æ¬¡æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰å›ç­”å†…å®¹éƒ½æ¥è‡ªæä¾›çš„ä¸Šä¸‹æ–‡"
    logger.info("æç¤ºæ„å»ºå®Œæˆ")

    # è°ƒç”¨LLMç”Ÿæˆå›ç­”
    try:
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±ä¿¡æ¯é—®ç­”åŠ©æ‰‹ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ç”µå½±çŸ¥è¯†å’Œå‡ºè‰²çš„é—®ç­”æŠ€å·§ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå‡†ç¡®ã€å…¨é¢åœ°å›ç­”ç”¨æˆ·çš„ç”µå½±ç›¸å…³é—®é¢˜ã€‚å›ç­”åº”å…·æœ‰æ·±åº¦å’Œæ´å¯ŸåŠ›ï¼Œèƒ½å¤Ÿç»¼åˆåˆ†æå¤šç§ä¿¡æ¯æºï¼Œå¹¶ä»¥æ¸…æ™°ã€ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°ã€‚å¯¹äºå¤æ‚é—®é¢˜ï¼Œåº”æä¾›è¯¦ç»†è§£é‡Šå’Œä¾æ®ã€‚"},
            {"role": "user", "content": prompt}
        ]
        t_llm_start = time.time()
        response = llm.call(messages)
        t_llm_end = time.time()
        answer = response["content"].strip()
        llm_time = format_time(t_llm_end - t_llm_start)
        logger.info(f"LLMè°ƒç”¨å®Œæˆï¼Œè€—æ—¶{llm_time}")
        print(f"ğŸ’¡ LLMè°ƒç”¨è€—æ—¶: {t_llm_end - t_llm_start:.2f}s")

        # å¢å¼ºæ ¼å¼åŒ–å›ç­”
        formatted_answer = answer
        # 1. ç§»é™¤å¤šä½™ç©ºè¡Œ
        formatted_answer = re.sub(r'(\n\n)+', '\n\n', formatted_answer)
        # 2. ä¼˜åŒ–æ ‡é¢˜æ ¼å¼ï¼ˆå‡è®¾æ ‡é¢˜ä»¥æ•°å­—æˆ–ç¬¦å·å¼€å¤´ï¼‰
        formatted_answer = re.sub(r'^([0-9]+\.|-|\*)\s+', r'\1 ', formatted_answer, flags=re.MULTILINE)
        # 3. ç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„ç©ºè¡Œ
        formatted_answer = re.sub(r'([^\n])\n([^\n])', r'\1\n\n\2', formatted_answer)
        # 4. ç§»é™¤é¦–å°¾ç©ºç™½
        formatted_answer = formatted_answer.strip()

        # æ„å»ºè¿”å›ä¿¡æ¯
        answer_info = {
            "answer": formatted_answer,
            "context_length": len(context),
            "node_count": len(reranked_nodes),
            "llm_time": llm_time
        }
        return formatted_answer, answer_info
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
        return "å¾ˆæŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºé”™ã€‚", {"error": str(e)}


def evaluate_and_save(eval_data: List[Dict[str, Any]], user_input: str, answer: str, answer_info: Dict[str, Any], eval_data_path: str = "eval_data.json") -> None:
    """
    è¯„ä¼°å›ç­”å¹¶ä¿å­˜åˆ°è¯„æµ‹æ•°æ®
    
    Args:
        eval_data: è¯„æµ‹æ•°æ®åˆ—è¡¨
        user_input: ç”¨æˆ·è¾“å…¥
        answer: ç”Ÿæˆçš„å›ç­”
        answer_info: å›ç­”ç›¸å…³ä¿¡æ¯
        eval_data_path: è¯„æµ‹æ•°æ®ä¿å­˜è·¯å¾„
    """
    try:
        # ç”Ÿæˆè¯„ä¼°ç»“æœ
        evaluation = get_checked_answer(user_input, answer)
        logger.info(f"è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {evaluation.get('total_score', 0)}")

        # æ„å»ºè¯„æµ‹æ•°æ®æ¡ç›®
        # ç¡®ä¿reranked_nodeså·²å®šä¹‰
        contexts = []
        if 'reranked_nodes' in locals() or 'reranked_nodes' in globals():
            contexts = [_nws_text(node) for node in reranked_nodes]
        
        eval_item = {
            "question": user_input,
            "answer": answer,
            "contexts": contexts,
            "evaluation": evaluation,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "context_length": answer_info.get("context_length", 0),
            "node_count": answer_info.get("node_count", 0),
            "llm_time": answer_info.get("llm_time", "0s")
        }

        # æ·»åŠ åˆ°è¯„æµ‹æ•°æ®
        eval_data.append(eval_item)

        # ä¿å­˜è¯„æµ‹æ•°æ®
        with open(eval_data_path, "w", encoding="utf-8") as f:
            json.dump(eval_data, f, ensure_ascii=False, indent=2)
        logger.info(f"è¯„æµ‹æ•°æ®å·²ä¿å­˜ï¼Œå…±{len(eval_data)}æ¡")
    except Exception as e:
        logger.error(f"è¯„ä¼°æˆ–ä¿å­˜å¤±è´¥ï¼š{e}")
        print(f"âš ï¸ è¯„ä¼°æˆ–ä¿å­˜å¤±è´¥ï¼š{e}")


def main():
    """
    ä¸»å‡½æ•°å…¥å£
    """
    # 1. åŠ è½½é…ç½®
    config = load_config()
    if not config:
        return
    
    # 2. éªŒè¯é…ç½®
    if not validate_config(config):
        return
    
    # 3. åˆå§‹åŒ–æœåŠ¡
    try:
        llm, main_kb, temp_kb, node_parser, kg_loader = init_services(config)
    except Exception as e:
        logger.error(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return

    # æ£€æŸ¥çŸ¥è¯†å›¾è°±æ˜¯å¦åŠ è½½æˆåŠŸ
    kg_available = hasattr(kg_loader, 'G') and kg_loader.G is not None
    if kg_available:
        print("âœ… çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸï¼Œå¯ç”¨äºå¢å¼ºå›ç­”è´¨é‡")
    else:
        print("âš ï¸ çŸ¥è¯†å›¾è°±æœªåŠ è½½æˆ–åŠ è½½å¤±è´¥")
    
    # åŠ è½½ä¸´æ—¶åº“æ£€ç´¢å™¨
    temp_meta_ret, temp_review_ret, temp_reranker = load_temp_retrievers(temp_kb)
    
    # åŠ è½½è¯„æµ‹æ•°æ®
    eval_data = load_eval_data()
    
    print(f"ä¸´åº“çŠ¶æ€ï¼šmeta_ret={bool(temp_meta_ret)}, review_ret={bool(temp_review_ret)}")
    print("å‘½ä»¤ï¼šexit / clear / upload <path> / remove temp / build_temp_index")
    
    chat_history = []
    main_meta_ret, main_review_ret, main_reranker = main_kb.get_retrievers()
    
    while True:
        user_input = input("\né—®é¢˜ï¼š").strip()
        if not user_input:
            continue
        
        # å¤„ç†å‘½ä»¤
        if process_command(user_input, temp_kb):
            # é‡æ–°åŠ è½½ä¸´æ—¶åº“æ£€ç´¢å™¨
            temp_meta_ret, temp_review_ret, temp_reranker = load_temp_retrievers(temp_kb)
            continue
        elif user_input.lower() == "exit":
            break
        
        # è®°å½•å¼€å§‹æ—¶é—´
        t_start = time.time()
        
        # åˆ†ææŸ¥è¯¢
        translated = analyze_query(llm, user_input, chat_history, kg_loader)
        
        # æ£€ç´¢æ–‡æ¡£
        meta_nodes, rev_nodes, temp_meta_nodes, temp_review_nodes = retrieve_documents(
            translated, main_meta_ret, main_review_ret, temp_meta_ret, temp_review_ret
        )
        
        # å¯è§†åŒ–å¬å›ç»“æœ
        visualize_retrieved_nodes(meta_nodes, "ä¸»åº“å…ƒæ•°æ®")
        visualize_retrieved_nodes(rev_nodes, "ä¸»åº“è¯„è®º")
        visualize_retrieved_nodes(temp_meta_nodes, "ä¸´æ—¶åº“å…ƒæ•°æ®")
        visualize_retrieved_nodes(temp_review_nodes, "ä¸´æ—¶åº“è¯„è®º")
        
        # ä¸ºä¸åŒåº“çš„èŠ‚ç‚¹æ·»åŠ æƒé‡æ ‡è®°
        weighted_nodes = []
        # ä¸»åº“å…ƒæ•°æ®èŠ‚ç‚¹ - åŸºç¡€æƒé‡1.0
        for node in meta_nodes:
            weighted_nodes.append((node, 1.0))

        # ä¸»åº“è¯„è®ºèŠ‚ç‚¹ - åŸºç¡€æƒé‡1.0
        for node in rev_nodes:
            weighted_nodes.append((node, 1.0))

        # ä¸´æ—¶åº“å…ƒæ•°æ®èŠ‚ç‚¹ - åŸºç¡€æƒé‡0.8
        for node in temp_meta_nodes:
            weighted_nodes.append((node, 0.8))

        # ä¸´æ—¶åº“è¯„è®ºèŠ‚ç‚¹ - åŸºç¡€æƒé‡0.8
        for node in temp_review_nodes:
            weighted_nodes.append((node, 0.8))
        
        # é‡æ’åºå¹¶ç”Ÿæˆå›ç­”
        answer, answer_info = rerank_and_answer(llm, user_input, weighted_nodes, main_reranker, temp_reranker, kg_loader)
        
        # è®°å½•ç»“æŸæ—¶é—´
        t_end = time.time()
        total_time = format_time(t_end - t_start)
        logger.info(f"æ€»è€—æ—¶: {total_time}")
        print(f"â±ï¸ æ€»è€—æ—¶: {t_end - t_start:.2f}s")
        
        # æ‰“å°å›ç­”
        print("\nâœ¨ å›ç­”:")
        print(answer)
        
        # æ›´æ–°å¯¹è¯å†å²
        chat_history.append({"role": "user", "content": user_input})
        chat_history.append({"role": "assistant", "content": answer})
        # é™åˆ¶å†å²é•¿åº¦
        if len(chat_history) > 20:  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            chat_history = chat_history[-20:]
        
        # è¯„ä¼°å¹¶ä¿å­˜
        evaluate_and_save(eval_data, user_input, answer, answer_info)

if __name__ == "__main__":
    main()
