import json
import re
from textwrap import dedent
import time

DEFAULT_SCORE_FIELDS = [
    ("relevance", "å›ç­”æ˜¯å¦åˆ‡é¢˜"),
    ("detail", "æ˜¯å¦åŒ…å«å…·ä½“ç”µå½±ä¿¡æ¯ï¼ˆå¦‚åç§°ã€è¯„åˆ†ã€é£æ ¼ï¼‰"),
    ("truth", "æ˜¯å¦å†…å®¹çœŸå®ï¼Œæ— ç¼–é€ "),
    ("logic", "æ˜¯å¦è¯­è¨€æµç•…ã€æœ‰æ¡ç†"),
    ("context_usage", "æ˜¯å¦å……åˆ†åˆ©ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")  # æ–°å¢ç»´åº¦
]

DEFAULT_SCORE_THRESHOLD = 20  # 5ä¸ªç»´åº¦æ»¡åˆ†25ï¼Œ20åˆ†ä¸ºåŠæ ¼çº¿


def extract_json_block(s: str) -> str:
    """ä»å­—ç¬¦ä¸²ä¸­æå–JSONä»£ç å—"""
    # å°è¯•åŒ¹é…æ ‡å‡†JSONå—
    match = re.search(r"```json(.*?)```", s, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # å°è¯•åŒ¹é…æ²¡æœ‰è¯­è¨€æ ‡è®°çš„ä»£ç å—
    match = re.search(r"```(.*?)```", s, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # å°è¯•åŒ¹é…å¯èƒ½çš„JSONç»“æ„
    match = re.search(r"\{.*?\}", s, re.DOTALL)
    if match:
        return match.group(0).strip()
    
    return s.strip()


def make_eval_prompt(question, answer, score_fields=None):
    """ä¿®å¤JSONæ¨¡æ¿å°¾éƒ¨é€—å·é—®é¢˜å¹¶æ·»åŠ é‡åŒ–è¯„åˆ†æ ‡å‡†"""
    score_fields = score_fields or DEFAULT_SCORE_FIELDS
    # é‡åŒ–è¯„åˆ†æ ‡å‡†
    score_standards = """
    è¯„åˆ†æ ‡å‡†ï¼ˆæ¯é¡¹1-5åˆ†ï¼‰ï¼š
    - ç›¸å…³æ€§ï¼š1åˆ†ï¼ˆå®Œå…¨åç¦»é—®é¢˜ï¼‰ï¼Œ3åˆ†ï¼ˆéƒ¨åˆ†ç›¸å…³ï¼‰ï¼Œ5åˆ†ï¼ˆå®Œå…¨è´´åˆé—®é¢˜ï¼‰
    - ç»†èŠ‚ï¼š1åˆ†ï¼ˆæ— å…·ä½“ä¿¡æ¯ï¼‰ï¼Œ3åˆ†ï¼ˆå«éƒ¨åˆ†ä¿¡æ¯å¦‚åç§°ï¼‰ï¼Œ5åˆ†ï¼ˆå«åç§°ã€è¯„åˆ†ã€å¹´ä»½ç­‰å®Œæ•´ä¿¡æ¯ï¼‰
    - çœŸå®æ€§ï¼š1åˆ†ï¼ˆå­˜åœ¨ç¼–é€ å†…å®¹ï¼‰ï¼Œ3åˆ†ï¼ˆéƒ¨åˆ†ä¿¡æ¯ä¸å‡†ç¡®ï¼‰ï¼Œ5åˆ†ï¼ˆå®Œå…¨çœŸå®æ— ç¼–é€ ï¼‰
    - é€»è¾‘ï¼š1åˆ†ï¼ˆæ··ä¹±æ— æ¡ç†ï¼‰ï¼Œ3åˆ†ï¼ˆåŸºæœ¬æµç•…ï¼‰ï¼Œ5åˆ†ï¼ˆæµç•…æœ‰æ¡ç†ï¼‰
    - ä¸Šä¸‹æ–‡ä½¿ç”¨ï¼š1åˆ†ï¼ˆæœªä½¿ç”¨ä¸Šä¸‹æ–‡ï¼‰ï¼Œ3åˆ†ï¼ˆéƒ¨åˆ†ä½¿ç”¨ï¼‰ï¼Œ5åˆ†ï¼ˆå……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
    """
    fields_str = "\n".join([
        f"{i + 1}. {k}ï¼š{desc}"
        for i, (k, desc) in enumerate(score_fields)
    ])
    # åŠ¨æ€ç”Ÿæˆå­—æ®µï¼Œæ— å°¾éƒ¨é€—å·
    fields_json = ",\n  ".join([f"\"{k}\": 5" for k, _ in score_fields])
    eval_prompt = dedent(f"""
    ä½ æ˜¯ä¸€ä¸ªç”µå½±é—®ç­”è´¨é‡è¯„ä¼°åŠ©æ‰‹ã€‚
    {score_standards}
    è¯·ä»ä»¥ä¸‹ç»´åº¦å¯¹å›ç­”æ‰“åˆ†ï¼ˆæ¯é¡¹æ»¡åˆ†5åˆ†ï¼‰ï¼š
    {fields_str}
    è‹¥å¾—åˆ†ä½äºåŠæ ¼çº¿ï¼Œè¯·æä¾›æ”¹è¿›å»ºè®®å’Œæ”¹å†™ç‰ˆæœ¬ã€‚
    **ä¸¥æ ¼åªè¾“å‡ºå¦‚ä¸‹JSONæ ¼å¼ï¼Œç”¨```jsonåŒ…è£¹ï¼Œæ— å¤šä½™æ–‡å­—ï¼**
    ```json
    {{
      {fields_json}
      {'' if not fields_json else ','}  # ä»…å½“æœ‰å­—æ®µæ—¶æ‰åŠ é€—å·
      "suggestion": "å»ºè®®æ”¹è¿›ç‚¹",
      "rewrite": "æ”¹å†™åçš„ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰"
    }}
    é—®é¢˜ï¼š{question}
    å›ç­”ï¼š{answer}""").strip ()
    return eval_prompt


def evaluate_answer(llm, question: str, answer: str, contexts: list = None,
                    score_fields=None, score_threshold=None) -> dict:
    """è¯„ä¼°å›ç­”è´¨é‡ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒä¸Šä¸‹æ–‡è¯„ä¼°ï¼‰"""
    score_fields = score_fields or DEFAULT_SCORE_FIELDS
    score_threshold = score_threshold or DEFAULT_SCORE_THRESHOLD
    eval_prompt = make_eval_prompt(question, answer, score_fields)
    messages = [
        {"role": "system", "content": "ä½ æ˜¯è¯„åˆ†å’Œåé¦ˆåŠ©æ‰‹"},
        {"role": "user", "content": eval_prompt}
    ]

    # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
    try:
        result = llm.call(messages)
        result_content = result["content"] if isinstance(result, dict) else result
        result_json = extract_json_block(result_content)
    except Exception as e:
        print(f"âš ï¸ LLMè¯„ä¼°è°ƒç”¨å¤±è´¥ï¼š{e}")
        # æ„å»ºé»˜è®¤ç»“æœ
        base = {k: 1 for k, _ in score_fields}
        base.update({
            "suggestion": "LLMè¯„ä¼°è°ƒç”¨å¤±è´¥",
            "rewrite": ""
        })
        return base

    # è§£æè¯„ä¼°ç»“æœ
    try:
        parsed = json.loads(result_json)
        # è¡¥å…¨ç¼ºå¤±å­—æ®µï¼ˆå®¹é”™ï¼‰
        for k, _ in score_fields:
            if k not in parsed:
                parsed[k] = 1  # ç¼ºå¤±å­—æ®µæŒ‰æœ€ä½åˆ†å¤„ç†
        parsed.setdefault("suggestion", "")
        parsed.setdefault("rewrite", "")

        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæä¾›äº†ä¸Šä¸‹æ–‡ï¼Œä¿æŒLLMçš„ä¸Šä¸‹æ–‡ä½¿ç”¨è¯„åˆ†
        if contexts and "context_usage" in parsed:
            # ç®€åŒ–å¤„ç†ï¼Œä¿æŒLLMçš„åŸå§‹è¯„åˆ†
            pass

        return parsed
    except Exception as e:
        print(f"âš ï¸ è§£æè¯„åˆ†JSONå¤±è´¥ï¼š{e}")
        print(f"åŸå§‹è¾“å‡ºï¼š{repr(result_content)}")
        # æ„å»ºé»˜è®¤ç»“æœ
        base = {k: 1 for k, _ in score_fields}
        base.update({
            "suggestion": "è¯„åˆ†JSONè§£æå¤±è´¥",
            "rewrite": ""
        })
        return base


def get_checked_answer(
        llm, question: str, messages: list, contexts: list = None,
        score_fields=None, score_threshold=None, show_think=False
) -> str:
    """æ£€æŸ¥å›ç­”è´¨é‡å¹¶è¿”å›æœ€ä¼˜ç‰ˆæœ¬ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒä¸Šä¸‹æ–‡è¯„ä¼°ï¼‰"""
    try:
        if show_think:
            response = llm.call(messages, return_think=True)
            raw_answer = response["content"]
            think = response["think"]
            if think:
                print(f"\nğŸ§  LLMæ€è€ƒè¿‡ç¨‹ï¼š\n{think}\n")
        else:
            response = llm.call(messages)
            raw_answer = response["content"] if isinstance(response, dict) else response

        eval_result = evaluate_answer(
            llm, question, raw_answer, contexts,
            score_fields=score_fields,
            score_threshold=score_threshold
        )

        # è®¡ç®—æ€»åˆ†
        total = sum([
            eval_result.get(k, 0)
            for k, _ in (score_fields or DEFAULT_SCORE_FIELDS)
        ])
        pass_threshold = score_threshold or DEFAULT_SCORE_THRESHOLD
        print(f"ğŸ§ª å›ç­”è´¨é‡è¯„åˆ†ï¼š{total}/{len(score_fields or DEFAULT_SCORE_FIELDS) * 5}")
        print(f"ğŸ“Œ æ”¹è¿›å»ºè®®ï¼š{eval_result.get('suggestion', 'æ— ')}")

        # å†³å®šè¿”å›åŸå§‹å›ç­”è¿˜æ˜¯æ”¹å†™ç‰ˆæœ¬
        if total < pass_threshold and eval_result.get("rewrite"):
            print("âš ï¸ å›ç­”è´¨é‡è¾ƒä½ï¼Œä½¿ç”¨è‡ªåŠ¨æ”¹å†™ç‰ˆæœ¬")
            return eval_result["rewrite"]
        else:
            # é¢å¤–ä¼˜åŒ–ï¼šå¦‚æœå›ç­”è¿‡é•¿ï¼Œè¿›è¡Œç²¾ç®€
            if len(raw_answer) > 1000:
                print("ğŸ”„ å›ç­”è¿‡é•¿ï¼Œè¿›è¡Œç²¾ç®€")
                # è°ƒç”¨LLMè¿›è¡Œç²¾ç®€
                shorten_prompt = f"è¯·ç²¾ç®€ä»¥ä¸‹å›ç­”ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼Œæ§åˆ¶åœ¨500å­—ä»¥å†…ï¼š\n{raw_answer}"
                shorten_messages = [
                    {"role": "system", "content": "ä½ æ˜¯æ–‡æœ¬ç²¾ç®€åŠ©æ‰‹"},
                    {"role": "user", "content": shorten_prompt}
                ]
                try:
                    shorten_response = llm.call(shorten_messages)
                    return shorten_response["content"] if isinstance(shorten_response, dict) else shorten_response
                except Exception as e:
                    print(f"âš ï¸ ç²¾ç®€å›ç­”å¤±è´¥ï¼š{e}")
            return raw_answer
    except Exception as e:
        print(f"âš ï¸ å›ç­”æ£€æŸ¥å¤±è´¥ï¼š{e}")
        # å°è¯•ç›´æ¥è¿”å›åŸå§‹å›ç­”ï¼Œè€Œä¸æ˜¯å›ºå®šé”™è¯¯æ¶ˆæ¯
        try:
            if 'raw_answer' in locals() and raw_answer:
                return raw_answer
        except:
            pass
        return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆç¬¦åˆè´¨é‡çš„å›ç­”"


import logging
import re
from typing import Dict, Any, List, Tuple
from core.llm_service import LLMService
from core.utils import format_time

# åˆå§‹åŒ–æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("system.log", encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# å®šä¹‰è¯„åˆ†æ ‡å‡†
SCORING_CRITERIA = [
    {
        "id": "relevance",
        "name": "ç›¸å…³æ€§",
        "description": "å›ç­”å†…å®¹ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³ç¨‹åº¦",
        "weight": 0.3,
        "levels": [
            {"score": 0, "description": "å®Œå…¨ä¸ç›¸å…³ï¼Œæœªæ¶‰åŠé—®é¢˜æ ¸å¿ƒ"},
            {"score": 1, "description": "éƒ¨åˆ†ç›¸å…³ï¼Œä½†æœªå›ç­”ä¸»è¦é—®é¢˜"},
            {"score": 2, "description": "å¤§éƒ¨åˆ†ç›¸å…³ï¼ŒåŸºæœ¬å›ç­”äº†ä¸»è¦é—®é¢˜"},
            {"score": 3, "description": "å®Œå…¨ç›¸å…³ï¼Œç²¾å‡†å›ç­”äº†æ‰€æœ‰æ ¸å¿ƒé—®é¢˜"}
        ]
    },
    {
        "id": "accuracy",
        "name": "å‡†ç¡®æ€§",
        "description": "å›ç­”å†…å®¹çš„äº‹å®å‡†ç¡®æ€§",
        "weight": 0.3,
        "levels": [
            {"score": 0, "description": "å­˜åœ¨ä¸¥é‡äº‹å®é”™è¯¯"},
            {"score": 1, "description": "å­˜åœ¨éƒ¨åˆ†äº‹å®é”™è¯¯"},
            {"score": 2, "description": "åŸºæœ¬å‡†ç¡®ï¼Œæ— é‡å¤§äº‹å®é”™è¯¯"},
            {"score": 3, "description": "å®Œå…¨å‡†ç¡®ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½æœ‰å¯é ä¾æ®"}
        ]
    },
    {
        "id": "completeness",
        "name": "å®Œæ•´æ€§",
        "description": "å›ç­”å†…å®¹çš„å…¨é¢ç¨‹åº¦",
        "weight": 0.2,
        "levels": [
            {"score": 0, "description": "å›ç­”ä¸å®Œæ•´ï¼Œé—æ¼å…³é”®ä¿¡æ¯"},
            {"score": 1, "description": "éƒ¨åˆ†å®Œæ•´ï¼Œé—æ¼å°‘é‡å…³é”®ä¿¡æ¯"},
            {"score": 2, "description": "åŸºæœ¬å®Œæ•´ï¼ŒåŒ…å«å¤§éƒ¨åˆ†å…³é”®ä¿¡æ¯"},
            {"score": 3, "description": "å®Œå…¨å®Œæ•´ï¼ŒåŒ…å«æ‰€æœ‰ç›¸å…³å…³é”®ä¿¡æ¯"}
        ]
    },
    {
        "id": "clarity",
        "name": "æ¸…æ™°åº¦",
        "description": "å›ç­”çš„ç»„ç»‡å’Œè¡¨è¾¾æ¸…æ™°åº¦",
        "weight": 0.1,
        "levels": [
            {"score": 0, "description": "è¡¨è¾¾æ··ä¹±ï¼Œéš¾ä»¥ç†è§£"},
            {"score": 1, "description": "éƒ¨åˆ†æ¸…æ™°ï¼Œå­˜åœ¨æ­§ä¹‰"},
            {"score": 2, "description": "åŸºæœ¬æ¸…æ™°ï¼Œæ˜“äºç†è§£"},
            {"score": 3, "description": "éå¸¸æ¸…æ™°ï¼Œç»“æ„ä¸¥è°¨ï¼Œè¡¨è¾¾æµç•…"}
        ]
    },
    {
        "id": "objectivity",
        "name": "å®¢è§‚æ€§",
        "description": "å›ç­”çš„å®¢è§‚ä¸­ç«‹ç¨‹åº¦",
        "weight": 0.1,
        "levels": [
            {"score": 0, "description": "ä¸»è§‚è‡†æ–­ï¼Œç¼ºä¹ä¾æ®"},
            {"score": 1, "description": "éƒ¨åˆ†ä¸»è§‚ï¼Œä¾æ®ä¸è¶³"},
            {"score": 2, "description": "åŸºæœ¬å®¢è§‚ï¼Œæœ‰ä¸€å®šä¾æ®"},
            {"score": 3, "description": "å®Œå…¨å®¢è§‚ï¼Œä¾æ®å……åˆ†"}
        ]
    }
]


def generate_evaluation_prompt(question: str, answer: str) -> str:
    """
    ç”Ÿæˆè¯„ä¼°æç¤º
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        answer: ç”Ÿæˆçš„å›ç­”
    
    Returns:
        str: è¯„ä¼°æç¤º
    """
    criteria_text = "\n".join([
        f"{i+1}. {criterion['name']} ({criterion['weight']*100}%): {criterion['description']}\n" + \
        "   è¯„åˆ†æ ‡å‡†:\n" + \
        "   \n".join([f"   - {level['score']}åˆ†: {level['description']}" for level in criterion['levels']])
        for i, criterion in enumerate(SCORING_CRITERIA)
    ])

    prompt = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”è¯„ä¼°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¯„åˆ†æ ‡å‡†è¯„ä¼°å›ç­”çš„è´¨é‡ã€‚\n\n"
    prompt += f"ç”¨æˆ·é—®é¢˜: {question}\n\n"
    prompt += f"ç”Ÿæˆçš„å›ç­”: {answer}\n\n"
    prompt += f"è¯„åˆ†æ ‡å‡†:\n{criteria_text}\n\n"
    prompt += "è¦æ±‚:\n"
    prompt += "1. ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†è¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦ç»™å‡ºå…·ä½“åˆ†æ•°å’Œç®€çŸ­ç†ç”±\n"
    prompt += "2. è®¡ç®—åŠ æƒæ€»åˆ†ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰\n"
    prompt += "3. æœ€åç»™å‡ºæ€»ä½“è¯„ä»·å’Œæ”¹è¿›å»ºè®®\n"
    prompt += "4. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸ºJSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:\n"
    prompt += "   - scores: å„ç»´åº¦è¯„åˆ†çš„å­—å…¸ï¼ˆé”®ä¸ºç»´åº¦idï¼Œå€¼ä¸ºåˆ†æ•°ï¼‰\n"
    prompt += "   - reasons: å„ç»´åº¦è¯„åˆ†ç†ç”±çš„å­—å…¸ï¼ˆé”®ä¸ºç»´åº¦idï¼Œå€¼ä¸ºç†ç”±ï¼‰\n"
    prompt += "   - total_score: åŠ æƒæ€»åˆ†\n"
    prompt += "   - overall_evaluation: æ€»ä½“è¯„ä»·\n"
    prompt += "   - improvement_suggestions: æ”¹è¿›å»ºè®®\n"
    prompt += "5. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯è¢«Pythonçš„json.loadsè§£æ\n"

    logger.info("è¯„ä¼°æç¤ºç”Ÿæˆå®Œæˆ")
    return prompt


def parse_evaluation_result(evaluation_text: str) -> Dict[str, Any]:
    """
    è§£æè¯„ä¼°ç»“æœ
    
    Args:
        evaluation_text: è¯„ä¼°æ–‡æœ¬
    
    Returns:
        Dict[str, Any]: è§£æåçš„è¯„ä¼°ç»“æœ
    """
    try:
        # æå–JSONéƒ¨åˆ†
        json_match = re.search(r'\{.*\}', evaluation_text, re.DOTALL)
        if not json_match:
            logger.error("è¯„ä¼°ç»“æœä¸­æœªæ‰¾åˆ°JSONæ ¼å¼å†…å®¹")
            return {
                "error": "è¯„ä¼°ç»“æœæ ¼å¼é”™è¯¯ï¼Œæœªæ‰¾åˆ°JSONå†…å®¹",
                "raw_text": evaluation_text
            }

        json_str = json_match.group()
        # æ›¿æ¢å¯èƒ½çš„è½¬ä¹‰é—®é¢˜
        json_str = json_str.replace('\n', '').replace('\r', '')
        # ä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é—®é¢˜
        json_str = re.sub(r'([{,])\s*([a-zA-Z0-9_]+)\s*:', r'\1"\2":', json_str)
        json_str = re.sub(r':\s*([a-zA-Z0-9_]+)\s*([},])', r':"\1"\2', json_str)
        json_str = re.sub(r':\s*([a-zA-Z0-9_]+)\s*$', r':"\1"', json_str)

        # è§£æJSON
        evaluation = json.loads(json_str)

        # éªŒè¯å¿…è¦å­—æ®µ
        required_fields = ["scores", "reasons", "total_score", "overall_evaluation", "improvement_suggestions"]
        for field in required_fields:
            if field not in evaluation:
                logger.warning(f"è¯„ä¼°ç»“æœç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                evaluation[field] = "" if field != "total_score" else 0

        logger.info(f"è¯„ä¼°ç»“æœè§£ææˆåŠŸï¼Œæ€»åˆ†: {evaluation.get('total_score', 0)}")
        return evaluation
    except json.JSONDecodeError as e:
        logger.error(f"è¯„ä¼°ç»“æœJSONè§£æå¤±è´¥: {e}")
        return {
            "error": f"JSONè§£æå¤±è´¥: {str(e)}",
            "raw_text": evaluation_text
        }
    except Exception as e:
        logger.error(f"è¯„ä¼°ç»“æœè§£æå¼‚å¸¸: {e}")
        return {
            "error": f"è§£æå¼‚å¸¸: {str(e)}",
            "raw_text": evaluation_text
        }


def calculate_weighted_score(scores: Dict[str, int]) -> float:
    """
    è®¡ç®—åŠ æƒæ€»åˆ†
    
    Args:
        scores: å„ç»´åº¦è¯„åˆ†
    
    Returns:
        float: åŠ æƒæ€»åˆ†
    """
    total = 0.0
    weight_sum = 0.0
    for criterion in SCORING_CRITERIA:
        criterion_id = criterion["id"]
        weight = criterion["weight"]
        if criterion_id in scores:
            total += scores[criterion_id] * weight
            weight_sum += weight
        else:
            logger.warning(f"ç¼ºå°‘ç»´åº¦è¯„åˆ†: {criterion_id}")

    if weight_sum == 0:
        logger.error("æ‰€æœ‰ç»´åº¦æƒé‡å’Œä¸º0ï¼Œæ— æ³•è®¡ç®—æ€»åˆ†")
        return 0.0

    # å½’ä¸€åŒ–åˆ°0-10åˆ†
    normalized_score = (total / weight_sum) * 10 / 3
    logger.info(f"åŠ æƒæ€»åˆ†è®¡ç®—å®Œæˆ: {normalized_score:.2f}")
    return round(normalized_score, 2)


def get_checked_answer(question: str, answer: str) -> Dict[str, Any]:
    """
    è·å–è¯„ä¼°åçš„å›ç­”
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        answer: ç”Ÿæˆçš„å›ç­”
    
    Returns:
        Dict[str, Any]: è¯„ä¼°ç»“æœ
    """
    try:
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        from core.llm_service import LLMService
        from core.utils import load_config

        # åŠ è½½é…ç½®
        config = load_config()
        if not config:
            logger.error("åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            config = {}

        # åˆå§‹åŒ–LLMæœåŠ¡
        llm = LLMService(config)

        # ç”Ÿæˆè¯„ä¼°æç¤º
        prompt = generate_evaluation_prompt(question, answer)

        # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
        logger.info("å¼€å§‹è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°")
        t_start = time.time()
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”è¯„ä¼°ä¸“å®¶ã€‚"},
            {"role": "user", "content": prompt}
        ]
        response = llm.call(messages)
        t_end = time.time()
        eval_time = format_time(t_end - t_start)
        logger.info(f"LLMè¯„ä¼°å®Œæˆï¼Œè€—æ—¶{eval_time}")

        # è§£æè¯„ä¼°ç»“æœ
        evaluation_text = response["content"].strip()
        evaluation = parse_evaluation_result(evaluation_text)

        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•è®¡ç®—åˆ†æ•°
        if "error" in evaluation:
            logger.warning("è¯„ä¼°ç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•è®¡ç®—åˆ†æ•°")
            # å‡è®¾ä¸€ä¸ªé»˜è®¤è¯„åˆ†
            default_scores = {criterion["id"]: 2 for criterion in SCORING_CRITERIA}
            evaluation = {
                "scores": default_scores,
                "reasons": {criterion["id"]: "è¯„ä¼°è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†" for criterion in SCORING_CRITERIA},
                "total_score": calculate_weighted_score(default_scores),
                "overall_evaluation": "è¯„ä¼°è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†",
                "improvement_suggestions": "æ— æ³•æä¾›å…·ä½“å»ºè®®ï¼Œå› ä¸ºè¯„ä¼°è§£æå¤±è´¥"
            }

        # æ·»åŠ è¯„ä¼°è€—æ—¶
        evaluation["eval_time"] = eval_time

        return evaluation
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        # è¿”å›é”™è¯¯ä¿¡æ¯
        return {
            "error": str(e),
            "scores": {},
            "reasons": {},
            "total_score": 0.0,
            "overall_evaluation": "è¯„ä¼°å¤±è´¥",
            "improvement_suggestions": "æ— æ³•æä¾›å»ºè®®ï¼Œå› ä¸ºè¯„ä¼°å¤±è´¥"
        }
